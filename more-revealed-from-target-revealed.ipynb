{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7056256,"sourceType":"competition"},{"sourceId":6924580,"sourceType":"datasetVersion","datasetId":3976011}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"debug = False\nn_rows_debug = 100000","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T06:22:56.390308Z","iopub.execute_input":"2023-11-29T06:22:56.390953Z","iopub.status.idle":"2023-11-29T06:22:56.421994Z","shell.execute_reply.started":"2023-11-29T06:22:56.390898Z","shell.execute_reply":"2023-11-29T06:22:56.421335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import date, datetime, timedelta\nimport matplotlib.pyplot as plt\nimport math\nimport collections\nimport time\nfrom copy import deepcopy\nimport seaborn as sns\nimport os, gc\nfrom tqdm import tqdm\nimport re\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\n\"\"\" SCIKIT-LEARN \"\"\"\nfrom sklearn import preprocessing\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix, classification_report, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, confusion_matrix, accuracy_score, mean_squared_error, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold, KFold, TimeSeriesSplit\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom collections import Counter, defaultdict\n\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\nimport joblib\n\npd.set_option('display.float_format', lambda x: '%.5f' % x) # No scientic notation\npd.set_option('display.max_columns', 100)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:23:02.588945Z","iopub.execute_input":"2023-11-29T06:23:02.589288Z","iopub.status.idle":"2023-11-29T06:23:05.297428Z","shell.execute_reply.started":"2023-11-29T06:23:02.589263Z","shell.execute_reply":"2023-11-29T06:23:05.296330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import polars as pl\n\ndef timestamp_UTC_conversion(train, date_col) :\n    \"\"\"  \"\"\"\n    \n    # To avoid bugs\n    if date_col == 'date' :\n        train = train.rename(columns = {'date' : 'date_'})\n        date_col = 'date_'\n            \n    # Transform\n    dt_transforms = [\n                     #pl.col(date_col).str.to_datetime(time_zone='UTC'), # UTC conversion to avoid different timezones\n                     (pl.col(date_col).str.to_datetime().dt.date()).alias('date'), #  Extract date\n                     pl.col(date_col).str.to_datetime().dt.time().alias('time'), # Extract time\n                    ]\n\n    # Apply transform\n    return  (pl.from_pandas(train)\n               .with_columns((dt_transforms))\n               .to_pandas()\n            )","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:23:10.580638Z","iopub.execute_input":"2023-11-29T06:23:10.581003Z","iopub.status.idle":"2023-11-29T06:23:10.774295Z","shell.execute_reply.started":"2023-11-29T06:23:10.580975Z","shell.execute_reply":"2023-11-29T06:23:10.773429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_date(train, date_col, prefixe = '', extract_features = False) :\n    \n    # Force to str (to avoid datetime error)\n    train[date_col] = train[date_col].astype(str)\n                \n    # Polar UTC conversion\n    train = timestamp_UTC_conversion(train, date_col)\n    \n    # Convert to string\n    train['date'] = train['date'].astype(str)\n    train['time'] = train['time'].astype(str)\n    train[date_col] = train['date'] + ' ' + train['time']\n    \n    # -----------------------------------------------------\n    # FEATURES EXTRACTION\n    \n    # Return\n    if not(extract_features) :\n        return train\n    \n    # Date features\n    train['year']  = (train['date'].apply(lambda x : x[:4]).astype(int)).astype(int)\n    train['month'] = train['date'].apply(lambda x : x[5:7]).astype(int)\n    train['day']   = train['date'].apply(lambda x : x[8:10]).astype(int)\n    \n    # Day of week\n    train['dayofweek'] = train[['year', 'month', 'day']].apply(lambda row : datetime(row['year'], row['month'], row['day']).weekday(), axis=1)\n    \n    # Time features\n    train['hour']    = train['time'].apply(lambda x : x[:2]).astype(int)\n    train['minutes'] = train['time'].apply(lambda x : x[3:5]).astype(int)\n    train['seconds'] = train['time'].apply(lambda x : x[6:7]).astype(int)\n    \n    # Rename\n    if len(prefixe) > 0 :\n        cols = ['year', 'month', 'day', 'hour', 'minutes', 'seconds']\n        train = train.rename(columns = {k : f\"{prefixe}_{k}\" for k in cols})\n\n    # Drop column\n    #train.drop(columns = [date_col], inplace=True)\n    \n    # Return\n    return train","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:24:16.762644Z","iopub.execute_input":"2023-11-29T06:24:16.762995Z","iopub.status.idle":"2023-11-29T06:24:16.772887Z","shell.execute_reply.started":"2023-11-29T06:24:16.762970Z","shell.execute_reply":"2023-11-29T06:24:16.771896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, print_info=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    \n    if print_info :\n        print('*'*50)\n        start_mem = df.memory_usage().sum() / 1024**2\n        print('Memory before : {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    if print_info :\n        end_mem = df.memory_usage().sum() / 1024**2\n        print('Memory after  : {:.2f} MB'.format(end_mem))\n        print('Decreased by  : {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n        print('*'*50 + '\\n')\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:24:25.164729Z","iopub.execute_input":"2023-11-29T06:24:25.165317Z","iopub.status.idle":"2023-11-29T06:24:25.175824Z","shell.execute_reply.started":"2023-11-29T06:24:25.165291Z","shell.execute_reply":"2023-11-29T06:24:25.174981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\")\nif debug :\n    print(\"DEBUG IS ON !\")\n    train = train.sample(n=n_rows_debug, random_state=12)\n\n# Show\nprint(train.shape)\ntrain.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:24:31.623711Z","iopub.execute_input":"2023-11-29T06:24:31.624040Z","iopub.status.idle":"2023-11-29T06:24:33.575518Z","shell.execute_reply.started":"2023-11-29T06:24:31.624018Z","shell.execute_reply":"2023-11-29T06:24:33.574571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pivot the training data to have a cleaner DataFrame where we can analyze the mean target values\n# organized by datetime and various categorical variables.\npivot_train = train.pivot_table(index='datetime',columns=['county','product_type','is_business','is_consumption'], values='target', aggfunc='mean')\n\n# Renaming columns for easier access and interpretation\npivot_train.columns = ['county{}_productType{}_isBusiness{}_isConsumption{}'.format(*col) for col in pivot_train.columns.values]\npivot_train.index = pd.to_datetime(pivot_train.index)\npivot_train.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:24:39.084552Z","iopub.execute_input":"2023-11-29T06:24:39.084993Z","iopub.status.idle":"2023-11-29T06:24:40.216907Z","shell.execute_reply.started":"2023-11-29T06:24:39.084959Z","shell.execute_reply":"2023-11-29T06:24:40.216131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_plot = pivot_train.copy()\ndf_plot = (df_plot - df_plot.min())/(df_plot.max() - df_plot.min())\ndf_plot_resampled_D = df_plot.resample('D').mean()\n\n# Plot the consumption data with alpha=0.1 \ndf_plot_resampled_D.loc['2022-7':].plot(alpha=0.1, color='gray', figsize=(15, 6), legend=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:25:16.240237Z","iopub.execute_input":"2023-11-29T06:25:16.240592Z","iopub.status.idle":"2023-11-29T06:25:17.631628Z","shell.execute_reply.started":"2023-11-29T06:25:16.240565Z","shell.execute_reply":"2023-11-29T06:25:17.630865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the relevant columns and time range\ncolumns_consumption_0 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption0')]\ncolumns_consumption_1 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption1')]\n\n# Create a single legend for each category\nplt.figure(figsize=(15, 6))\nplt.plot([], color='blue', label='is_Consumption = 1')\nplt.plot([], color='green', label='is_Consumption = 0')\nplt.legend()\n\n# Plot the data for is_Consumption = 0 in green\nfor column in columns_consumption_0:\n    df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='green', legend=False)\n\n# Plot the data for is_Consumption = 1 in blue\nfor column in columns_consumption_1:\n    df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='blue', legend=False)\n\n# Add a single legend to the plot\n#plt.legend()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:25:29.694517Z","iopub.execute_input":"2023-11-29T06:25:29.694858Z","iopub.status.idle":"2023-11-29T06:25:32.729088Z","shell.execute_reply.started":"2023-11-29T06:25:29.694831Z","shell.execute_reply":"2023-11-29T06:25:32.728094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the consumption_1 data\nconsumption_1 = df_plot.loc[:, df_plot.columns.str.contains('isConsumption1')]\n\n# Filter the data for the desired time period ('2023-5' onwards)\nconsumption_1_filtered = consumption_1.loc['2022-8']\n\n# Calculate the average values\naverage_values = consumption_1_filtered.mean(axis=1)\n\n# Create a figure and axis for the plot\nfig, ax = plt.subplots(figsize=(15, 6))\n\n# Plot the consumption data with alpha=0.1 in blue\nconsumption_1_filtered.plot(alpha=0.1, color='blue', ax=ax)\n\n# Plot the average values as a black bold line\naverage_values.plot(color='black', linewidth=2, ax=ax)\n\n# Set the legend\nax.legend(['Consumption Data', 'Average'])\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:25:39.064758Z","iopub.execute_input":"2023-11-29T06:25:39.065089Z","iopub.status.idle":"2023-11-29T06:25:40.369165Z","shell.execute_reply.started":"2023-11-29T06:25:39.065067Z","shell.execute_reply":"2023-11-29T06:25:40.368328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the average values\naverage_values = consumption_1.mean(axis=1)\n\n# Create a figure and axis with figsize\nfig, ax = plt.subplots(figsize=(15, 6))\n\n# Calculate and plot the autocorrelation\nplot_acf(average_values, lags=170, ax=ax)\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Plot')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:25:50.323384Z","iopub.execute_input":"2023-11-29T06:25:50.323704Z","iopub.status.idle":"2023-11-29T06:25:50.614816Z","shell.execute_reply.started":"2023-11-29T06:25:50.323680Z","shell.execute_reply":"2023-11-29T06:25:50.613949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Date processing + features extraction\nfor date_col in ['datetime'] :\n    train = process_date(train, date_col, extract_features = True)\n\n# Show\nprint(train.shape)\ntrain.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:26:33.672132Z","iopub.execute_input":"2023-11-29T06:26:33.672456Z","iopub.status.idle":"2023-11-29T06:27:03.896329Z","shell.execute_reply.started":"2023-11-29T06:26:33.672430Z","shell.execute_reply":"2023-11-29T06:27:03.895129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print datetimes in the train dataset (2021-09-01 -> 2023-05-31)\ntrain['datetime'].value_counts().to_frame().sort_index()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:27:03.898087Z","iopub.execute_input":"2023-11-29T06:27:03.898776Z","iopub.status.idle":"2023-11-29T06:27:04.085025Z","shell.execute_reply.started":"2023-11-29T06:27:03.898742Z","shell.execute_reply":"2023-11-29T06:27:04.083965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show target distribution\ndisplay(train['target'].describe())\ntrain['target'].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:27:04.085851Z","iopub.execute_input":"2023-11-29T06:27:04.086106Z","iopub.status.idle":"2023-11-29T06:27:04.507202Z","shell.execute_reply.started":"2023-11-29T06:27:04.086084Z","shell.execute_reply":"2023-11-29T06:27:04.506328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for day_shift in range(2, 8) :\n\n    # Add previous targets\n    train['data_block_id_shifted'] = train['data_block_id'] + day_shift\n    train = pd.merge(train,\n                     train[[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"time\", \"data_block_id_shifted\", \"target\"]].rename(columns = {\"data_block_id_shifted\":\"data_block_id\",\n                                                                                                                                                     \"target\" : f\"target_revealed_{day_shift}days_ago\",\n                                                                                                                                                    }),\n                     how = 'left',\n                     on = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"time\", \"data_block_id\"],\n                    )\n\n    # Fill NaN with -1\n    mask = train[f\"target_revealed_{day_shift}days_ago\"].isna()\n    train.loc[mask, f\"target_revealed_{day_shift}days_ago\"] = train.loc[mask, \"target\"]\n\n# Drop useless column\ntrain.drop(columns = ['data_block_id_shifted'], inplace=True)\n\n# Show\nprint(train.shape)\ntrain.tail(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:27:04.508783Z","iopub.execute_input":"2023-11-29T06:27:04.509048Z","iopub.status.idle":"2023-11-29T06:27:10.284592Z","shell.execute_reply.started":"2023-11-29T06:27:04.509026Z","shell.execute_reply":"2023-11-29T06:27:10.283764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"county = 15\nis_business = 1\nproduct_type = 3\nis_consumption = 1\n#mask = (train['county'] == county) & (train['is_business'] == is_business) & (train['product_type'] == product_type) & (train['is_consumption'] == is_consumption)\n#train.loc[mask, ['date', 'time', 'data_block_id', 'target', 'target_revealed']].tail(30","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:27:15.821516Z","iopub.execute_input":"2023-11-29T06:27:15.822204Z","iopub.status.idle":"2023-11-29T06:27:15.826123Z","shell.execute_reply.started":"2023-11-29T06:27:15.822180Z","shell.execute_reply":"2023-11-29T06:27:15.825250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train.copy()\n\n# Show\nprint(df.shape)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:27:24.395144Z","iopub.execute_input":"2023-11-29T06:27:24.395463Z","iopub.status.idle":"2023-11-29T06:27:24.699708Z","shell.execute_reply.started":"2023-11-29T06:27:24.395441Z","shell.execute_reply":"2023-11-29T06:27:24.698631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"electricity_prices = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\")\n\n# Date processing + features extraction\nfor date_col in ['forecast_date', 'origin_date'] :\n    electricity_prices = process_date(electricity_prices, date_col)\n    \n# Show\nprint(electricity_prices.shape)\nelectricity_prices.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:27:32.229876Z","iopub.execute_input":"2023-11-29T06:27:32.230256Z","iopub.status.idle":"2023-11-29T06:27:32.334483Z","shell.execute_reply.started":"2023-11-29T06:27:32.230229Z","shell.execute_reply":"2023-11-29T06:27:32.333394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Shift data_clock_id (during the submission phase, the data are available 2 days ago)\nelectricity_prices['data_block_id_shifted'] = electricity_prices['data_block_id'] + 2\n\n# Join\ndf = pd.merge(df,\n              electricity_prices[[\"time\", \"data_block_id_shifted\", \"euros_per_mwh\"]].rename(columns = {\"data_block_id_shifted\":\"data_block_id\"}),\n              how = 'left',\n              on = [\"time\", \"data_block_id\"],\n             )\n\n# Show\nprint(df.shape)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:27:40.182083Z","iopub.execute_input":"2023-11-29T06:27:40.182413Z","iopub.status.idle":"2023-11-29T06:27:40.482429Z","shell.execute_reply.started":"2023-11-29T06:27:40.182391Z","shell.execute_reply":"2023-11-29T06:27:40.481779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gas_prices = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\")\n\n# Date processing + features extraction\nfor date_col in ['forecast_date', 'origin_date'] :\n    gas_prices = process_date(gas_prices, date_col)\n    \n# Show\nprint(gas_prices.shape)\ngas_prices.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:27:50.940347Z","iopub.execute_input":"2023-11-29T06:27:50.940649Z","iopub.status.idle":"2023-11-29T06:27:50.971384Z","shell.execute_reply.started":"2023-11-29T06:27:50.940628Z","shell.execute_reply":"2023-11-29T06:27:50.970462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Shift data_clock_id (during the submission phase, the data are available 2 days ago)\ngas_prices['data_block_id_shifted'] = gas_prices['data_block_id'] + 2\n\n# Join\ncols = ['data_block_id_shifted', 'time', 'lowest_price_per_mwh', 'highest_price_per_mwh']\ndf = df.merge(gas_prices[cols].rename(columns = {'data_block_id_shifted' : 'data_block_id'}),\n             how='left',\n             on=[\"time\", \"data_block_id\"],\n             )\n\n# Show\nprint(df.shape)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:28:29.486190Z","iopub.execute_input":"2023-11-29T06:28:29.486553Z","iopub.status.idle":"2023-11-29T06:28:29.788060Z","shell.execute_reply.started":"2023-11-29T06:28:29.486526Z","shell.execute_reply":"2023-11-29T06:28:29.787103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"client = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\")\n\n# Rename\nclient = client.rename(columns = {'date' : 'datetime'})\n\n# Date processing + features extraction\nfor date_col in ['datetime'] :\n    client = process_date(client, date_col)\n    \n# Fill NaN\nclient.fillna(0, inplace=True)\n\n# Show\nprint(client.shape)\nclient.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:28:37.327445Z","iopub.execute_input":"2023-11-29T06:28:37.327765Z","iopub.status.idle":"2023-11-29T06:28:37.447870Z","shell.execute_reply.started":"2023-11-29T06:28:37.327742Z","shell.execute_reply":"2023-11-29T06:28:37.447072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Shift data_clock_id (during the submission phase, the data are available 2 days ago)\nclient['data_block_id_shifted'] = client['data_block_id'] + 2\n\n# Join\ndf = df.merge(client.drop(columns = ['date', 'datetime', 'data_block_id', 'time']).rename(columns = {'data_block_id_shifted' : 'data_block_id'}),\n             how='left',\n             on=['data_block_id', 'county', 'is_business', 'product_type'], # don't merge on time ! Client data are only available at 00:00:00\n             )\n\n# Show\nprint(df.shape)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:28:45.880753Z","iopub.execute_input":"2023-11-29T06:28:45.881076Z","iopub.status.idle":"2023-11-29T06:28:46.155423Z","shell.execute_reply.started":"2023-11-29T06:28:45.881054Z","shell.execute_reply":"2023-11-29T06:28:46.154417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location = pd.read_csv(\"/kaggle/input/fabiendaniels-mapping-locations-and-county-codes/county_lon_lats.csv\").drop(columns = [\"Unnamed: 0\"])\n\n# Convert to int to avoid float imprecision\nfor k in ['latitude', 'longitude'] :\n    location[k] = (10*location[k]).astype(int)\n\n# Show\nprint(location.shape)\nlocation.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:28:52.783249Z","iopub.execute_input":"2023-11-29T06:28:52.783588Z","iopub.status.idle":"2023-11-29T06:28:52.801831Z","shell.execute_reply.started":"2023-11-29T06:28:52.783560Z","shell.execute_reply":"2023-11-29T06:28:52.801237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_weather_info(h, location=location) :\n    \n    # Drop duplicates\n    h = h.drop_duplicates().reset_index(drop=True)\n\n    # Convert to int to avoid float imprecision\n    for k in ['latitude', 'longitude'] :\n        h[k] = (10*h[k]).astype(int)\n    \n    # Add location\n    h = pd.merge(h, location, how='left', on=['latitude', 'longitude'])\n    \n    # Fill NaN and force int\n    h['county'] = h['county'].fillna(-1).astype(int)\n\n    # Return\n    return h","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:29:00.432795Z","iopub.execute_input":"2023-11-29T06:29:00.433596Z","iopub.status.idle":"2023-11-29T06:29:00.439957Z","shell.execute_reply.started":"2023-11-29T06:29:00.433563Z","shell.execute_reply":"2023-11-29T06:29:00.438879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nhistorical_weather = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\")\n\n# Date processing + features extraction\nfor date_col in ['datetime'] :\n    historical_weather = process_date(historical_weather, date_col)\n    \n# Reduce memory usage to avoid OOM (Out OF Memory error)\n#historical_weather = reduce_memory_usage(historical_weather, print_info=True)\n\n# Show\nprint(historical_weather.shape)\nhistorical_weather.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:29:07.879910Z","iopub.execute_input":"2023-11-29T06:29:07.880231Z","iopub.status.idle":"2023-11-29T06:29:14.428333Z","shell.execute_reply.started":"2023-11-29T06:29:07.880207Z","shell.execute_reply":"2023-11-29T06:29:14.427495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add location\nhistorical_weather = process_weather_info(historical_weather)\n\n# Show\nprint(historical_weather.shape)\nhistorical_weather.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:29:14.651982Z","iopub.execute_input":"2023-11-29T06:29:14.652300Z","iopub.status.idle":"2023-11-29T06:29:16.495882Z","shell.execute_reply.started":"2023-11-29T06:29:14.652276Z","shell.execute_reply":"2023-11-29T06:29:16.494860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggregate information over latitude/longitude\ndict_agg = {'temperature' : ['min', 'mean', 'max', 'std'],\n            'dewpoint' : ['min', 'mean', 'max', 'std'],\n            'rain' : ['min', 'mean', 'max', 'std'],\n            'snowfall' : ['min', 'mean', 'max', 'std'],\n            'surface_pressure' : ['min', 'mean', 'max', 'std'],\n            'cloudcover_total' : ['min', 'mean', 'max', 'std'],\n            'cloudcover_low' : ['min', 'mean', 'max', 'std'],\n            'cloudcover_mid' : ['min', 'mean', 'max', 'std'],\n            'cloudcover_high' : ['min', 'mean', 'max', 'std'],\n            'windspeed_10m' : ['min', 'mean', 'max', 'std'],\n            'winddirection_10m' : ['min', 'mean', 'max', 'std'],\n            'shortwave_radiation' : ['min', 'mean', 'max', 'std'],\n            'direct_solar_radiation' : ['min', 'mean', 'max', 'std'],\n            'diffuse_radiation' : ['min', 'mean', 'max', 'std'],\n           }\n\n# Groupby\nkeys = ['county', 'datetime']\nhistorical_weather = historical_weather.groupby(keys).agg(dict_agg).reset_index()\n\n# Flatten columns names\nhistorical_weather.columns = ['_'.join([xx for xx in x if len(xx)>0]) for x in historical_weather.columns]\nhistorical_weather.columns = [x + '_h' if x not in keys else x for x in historical_weather.columns]\n\n# Show\nprint(historical_weather.shape)\nhistorical_weather.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:29:25.109492Z","iopub.execute_input":"2023-11-29T06:29:25.110044Z","iopub.status.idle":"2023-11-29T06:29:26.411028Z","shell.execute_reply.started":"2023-11-29T06:29:25.110016Z","shell.execute_reply":"2023-11-29T06:29:26.410191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Shift datetime\nhistorical_weather['datetime_shifted'] = (pd.to_datetime(historical_weather['datetime'].astype(str)) + pd.Timedelta(days=1, hours=13)).astype(str)\n\n# Join\ndf = df.merge(historical_weather.drop(columns = ['datetime']).rename(columns = {'datetime_shifted' : 'datetime'}),\n              how='left',\n              on=['county', 'datetime'],\n             )\n\n# Show\nprint(df.shape)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:29:31.465345Z","iopub.execute_input":"2023-11-29T06:29:31.465688Z","iopub.status.idle":"2023-11-29T06:29:32.623628Z","shell.execute_reply.started":"2023-11-29T06:29:31.465660Z","shell.execute_reply":"2023-11-29T06:29:32.622749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nforecast_weather = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\")\n\n# Date processing + features extraction\nfor date_col in ['forecast_datetime'] :\n    forecast_weather = process_date(forecast_weather, date_col)\n\n# Reduce memory usage to avoid OOM (Out OF Memory error)\n#forecast_weather = reduce_memory_usage(forecast_weather, print_info=True)\n\n# Show\nprint(forecast_weather.shape)\nforecast_weather.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:29:38.542900Z","iopub.execute_input":"2023-11-29T06:29:38.543567Z","iopub.status.idle":"2023-11-29T06:29:58.451531Z","shell.execute_reply.started":"2023-11-29T06:29:38.543536Z","shell.execute_reply":"2023-11-29T06:29:58.450456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add location\nforecast_weather = process_weather_info(forecast_weather)\n\n# Show\nprint(forecast_weather.shape)\nforecast_weather.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:02:49.604803Z","iopub.execute_input":"2023-11-29T07:02:49.605200Z","iopub.status.idle":"2023-11-29T07:02:58.255485Z","shell.execute_reply.started":"2023-11-29T07:02:49.605171Z","shell.execute_reply":"2023-11-29T07:02:58.254644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggregate information over latitude/longitude\ndict_agg = {'temperature' : ['min', 'mean', 'max', 'std'],\n            'dewpoint' : ['min', 'mean', 'max', 'std'],\n            'cloudcover_high' : ['min', 'mean', 'max', 'std'],\n            'cloudcover_low' : ['min', 'mean', 'max', 'std'],\n            'cloudcover_mid' : ['min', 'mean', 'max', 'std'],\n            'cloudcover_total' : ['min', 'mean', 'max', 'std'],\n            '10_metre_u_wind_component' : ['min', 'mean', 'max', 'std'],\n            '10_metre_v_wind_component' : ['min', 'mean', 'max', 'std'],\n            'direct_solar_radiation' : ['min', 'mean', 'max', 'std'],\n            'surface_solar_radiation_downwards' : ['min', 'mean', 'max', 'std'],\n            'snowfall' : ['min', 'mean', 'max', 'std'],\n            'total_precipitation' : ['min', 'mean', 'max', 'std'],\n           }\n\n# Groupby\nkeys = ['county', 'forecast_datetime']\nforecast_weather = forecast_weather.groupby(keys).agg(dict_agg).reset_index()\n\n# Flatten columns names\nforecast_weather.columns = ['_'.join([xx for xx in x if len(xx)>0]) for x in forecast_weather.columns]\nforecast_weather.columns = [x + '_f' if x not in keys else x for x in forecast_weather.columns]\n\n# Show\nprint(forecast_weather.shape)\nforecast_weather.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:03:37.153571Z","iopub.execute_input":"2023-11-29T07:03:37.153962Z","iopub.status.idle":"2023-11-29T07:03:39.211394Z","shell.execute_reply.started":"2023-11-29T07:03:37.153933Z","shell.execute_reply":"2023-11-29T07:03:39.210488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Join\ndf = df.merge(forecast_weather.rename(columns = {'forecast_datetime' : 'datetime'}),\n              how='left',\n              on=['county', 'datetime'],\n             )\n\n# Fill NaN\ndf = df.sort_values(by=['datetime']).reset_index(drop=True)\nfor k in df :\n    if k.endswith('_f') :\n        df[k] = df[k].ffill().bfill()\n\n# Show\nprint(df.shape)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:03:45.000232Z","iopub.execute_input":"2023-11-29T07:03:45.000568Z","iopub.status.idle":"2023-11-29T07:03:51.369245Z","shell.execute_reply.started":"2023-11-29T07:03:45.000544Z","shell.execute_reply":"2023-11-29T07:03:51.368400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort\ndf = df.sort_values(by=['datetime']).reset_index(drop=True)\n\n# Fill NaN\ndf.fillna(0, inplace=True)\n\n# Show\nprint(df.shape)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:03:51.370497Z","iopub.execute_input":"2023-11-29T07:03:51.370752Z","iopub.status.idle":"2023-11-29T07:03:59.102629Z","shell.execute_reply.started":"2023-11-29T07:03:51.370730Z","shell.execute_reply":"2023-11-29T07:03:59.101451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del historical_weather, forecast_weather\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:03:59.104164Z","iopub.execute_input":"2023-11-29T07:03:59.104437Z","iopub.status.idle":"2023-11-29T07:03:59.266652Z","shell.execute_reply.started":"2023-11-29T07:03:59.104416Z","shell.execute_reply":"2023-11-29T07:03:59.265211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_df(df, client, historical_weather, forecast_weather,\n              electricity_prices, gas_prices, sample_prediction) :\n\n    #################################################\n    # ⚡ ELECTRICITY FEATURES ⚡\n    #################################################\n\n    # Join\n    df = pd.merge(df,\n                  electricity_prices[[\"time\", \"euros_per_mwh\"]],\n                  how = 'left',\n                  on = [\"time\"],\n                 )\n\n    #################################################\n    # 🛢️ GAS FEATURES 🛢️\n    #################################################\n\n    # Join\n    cols = ['time', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n    df = df.merge(gas_prices[cols],\n                 how='left',\n                 on=[\"time\"],\n                 )\n\n    #################################################\n    # 🧑 CLIENT FEATURES 🧑\n    #################################################\n   \n    # Join\n    df = df.merge(client.drop(columns = ['datetime', 'date', 'time']),\n                 how='left',\n                 on=['county', 'is_business', 'product_type'],\n                 )\n\n    #################################################\n    # 🌤️ HISTORICAL WEATHER FEATURES 🌤️\n    #################################################\n    \n    # Aggregate information over latitude/longitude\n    dict_agg = {'temperature' : ['min', 'mean', 'max', 'std'],\n                'dewpoint' : ['min', 'mean', 'max', 'std'],\n                'rain' : ['min', 'mean', 'max', 'std'],\n                'snowfall' : ['min', 'mean', 'max', 'std'],\n                'surface_pressure' : ['min', 'mean', 'max', 'std'],\n                'cloudcover_total' : ['min', 'mean', 'max', 'std'],\n                'cloudcover_low' : ['min', 'mean', 'max', 'std'],\n                'cloudcover_mid' : ['min', 'mean', 'max', 'std'],\n                'cloudcover_high' : ['min', 'mean', 'max', 'std'],\n                'windspeed_10m' : ['min', 'mean', 'max', 'std'],\n                'winddirection_10m' : ['min', 'mean', 'max', 'std'],\n                'shortwave_radiation' : ['min', 'mean', 'max', 'std'],\n                'direct_solar_radiation' : ['min', 'mean', 'max', 'std'],\n                'diffuse_radiation' : ['min', 'mean', 'max', 'std'],\n               }\n\n    # Groupby\n    keys = ['county', 'datetime']\n    historical_weather = historical_weather.groupby(keys).agg(dict_agg).reset_index()\n\n    # Flatten columns names\n    historical_weather.columns = ['_'.join([xx for xx in x if len(xx)>0]) for x in historical_weather.columns]\n    historical_weather.columns = [x + '_h' if x not in keys else x for x in historical_weather.columns]\n\n    # Shift datetime\n    historical_weather['datetime'] = (pd.to_datetime(historical_weather['datetime'].astype(str)) + pd.Timedelta(days=1, hours=13)).astype(str)\n    \n    # Join\n    df = df.merge(historical_weather,\n                  how='left',\n                  on=['county', 'datetime'],\n                 )\n    \n    #################################################\n    # 🌤️ FORECAST WEATHER FEATURES 🌤️\n    #################################################\n    \n    # Aggregate information over datetime\n    dict_agg = {'temperature' : ['min', 'mean', 'max', 'std'],\n                'dewpoint' : ['min', 'mean', 'max', 'std'],\n                'cloudcover_high' : ['min', 'mean', 'max', 'std'],\n                'cloudcover_low' : ['min', 'mean', 'max', 'std'],\n                'cloudcover_mid' : ['min', 'mean', 'max', 'std'],\n                'cloudcover_total' : ['min', 'mean', 'max', 'std'],\n                '10_metre_u_wind_component' : ['min', 'mean', 'max', 'std'],\n                '10_metre_v_wind_component' : ['min', 'mean', 'max', 'std'],\n                'direct_solar_radiation' : ['min', 'mean', 'max', 'std'],\n                'surface_solar_radiation_downwards' : ['min', 'mean', 'max', 'std'],\n                'snowfall' : ['min', 'mean', 'max', 'std'],\n                'total_precipitation' : ['min', 'mean', 'max', 'std'],\n               }\n\n    # Groupby\n    keys = ['county', 'forecast_datetime']\n    forecast_weather = forecast_weather.groupby(keys).agg(dict_agg).reset_index()\n\n    # Flatten columns names\n    forecast_weather.columns = ['_'.join([xx for xx in x if len(xx)>0]) for x in forecast_weather.columns]\n    forecast_weather.columns = [x + '_f' if x not in keys else x for x in forecast_weather.columns]\n\n    # Join\n    df = df.merge(forecast_weather.rename(columns = {'forecast_datetime' : 'datetime'}),\n                  how='left',\n                  on=['county', 'datetime'],\n                 )\n\n    # Fill NaN\n    df = df.sort_values(by=['datetime']).reset_index(drop=True)\n    for k in df :\n        if k.endswith('_f') :\n            df[k] = df[k].ffill().bfill()\n    \n    #################################################\n    # ⚙️ FINAL PROCESSING ⚙️\n    #################################################\n    \n    # Sort\n    #df = df.sort_values(by=['datetime']).reset_index(drop=True)\n\n    # Fill NaN\n    df.fillna(0, inplace=True)\n\n    # Return\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:05:43.174185Z","iopub.execute_input":"2023-11-29T07:05:43.174502Z","iopub.status.idle":"2023-11-29T07:05:43.192756Z","shell.execute_reply.started":"2023-11-29T07:05:43.174479Z","shell.execute_reply":"2023-11-29T07:05:43.191783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forbidden_cols = ['target',\n                  'datetime',\n                  'date',\n                  'row_id',\n                  'data_block_id',\n                  'prediction_unit_id',\n                  \n                  # Useless feats\n                  'minutes',\n                  'snowfall_min_h',\n                  'rain_min_h',\n                  'seconds',\n                  'highest_price_per_mwh',\n                  'lowest_price_per_mwh',\n                  'snowfall_max_h',\n                  'snowfall_min_f',\n                  'cloudcover_mid_min_f',\n                  'cloudcover_high_min_f',\n                 ]\nnumeric_cols   = df.select_dtypes(include=np.number).columns.tolist()\n\nfeats = [x for x in numeric_cols if x not in forbidden_cols]\n\nprint(f\"{len(feats)} features.\")\ndf[feats].head(1)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:06:45.191234Z","iopub.execute_input":"2023-11-29T07:06:45.191803Z","iopub.status.idle":"2023-11-29T07:06:45.978545Z","shell.execute_reply.started":"2023-11-29T07:06:45.191778Z","shell.execute_reply":"2023-11-29T07:06:45.977971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fewer feats\nif False :\n    # Fewer feats\n    feats = ['day',\n             'month',\n             'euros_per_mwh',\n             'hour',\n             'dayofweek',\n             'county',\n             'target_revealed_6days_ago',\n             'target_revealed_5days_ago',\n             'target_revealed_4days_ago',\n             'target_revealed_3days_ago',\n             'target_revealed_2days_ago',\n             'year',\n             'is_consumption',\n             'is_business',\n             'product_type',\n             'eic_count',\n             'installed_capacity',\n             ]\n\n\nprint(f\"{len(feats)} features.\")\ndf[feats].head(1)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:07:08.088191Z","iopub.execute_input":"2023-11-29T07:07:08.088566Z","iopub.status.idle":"2023-11-29T07:07:08.480327Z","shell.execute_reply.started":"2023-11-29T07:07:08.088535Z","shell.execute_reply":"2023-11-29T07:07:08.479426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_params = {'boosting_type': 'gbdt',\n               'objective': 'regression',\n               'metric' : 'mean_absolute_error',\n               'importance_type': 'split',\n               'learning_rate': 0.08,\n               'n_estimators': 1000,\n               'max_depth': -1,\n               'min_child_samples': 120,\n               'num_leaves': 250,\n               'colsample_bytree': 0.85,\n               'subsample': 0.85,\n               'reg_alpha': 2,\n               'reg_lambda': 1,\n               'n_jobs': 2,\n               'random_state': 12,\n              }\n\nfit_params = {\"eval_metric\" : 'mean_absolute_error',\n              #\"eval_set\" : [(xtr, ytr), (xte, yte)],\n              \"eval_names\": ['train', 'test'],\n              \"categorical_feature\": 'auto'}","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:07:16.546520Z","iopub.execute_input":"2023-11-29T07:07:16.547413Z","iopub.status.idle":"2023-11-29T07:07:16.553074Z","shell.execute_reply.started":"2023-11-29T07:07:16.547377Z","shell.execute_reply":"2023-11-29T07:07:16.552109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nIterations = []\n\nfor date_limit in ['2023-01-01 00:00:00',\n                   '2023-02-01 00:00:00',\n                   '2023-03-01 00:00:00',\n                   '2023-04-01 00:00:00',\n                   #'2023-05-01 00:00:00',\n                  ] :\n\n    # Train and test\n    mask_train = (df['datetime'] <= date_limit)\n    xtr, ytr = df.loc[mask_train, feats], df.loc[mask_train, 'target']\n    xte, yte = df.loc[~mask_train, feats], df.loc[~mask_train, 'target']\n    \n    # Fit model\n    model = lgb.LGBMRegressor(**lgbm_params)\n    fit_params['eval_set'] = [(xtr, ytr), (xte, yte)]\n    model.fit(xtr, ytr, **fit_params, callbacks = [lgb.early_stopping(15, verbose=False),\n                                                   lgb.log_evaluation(0)])\n\n    # Add number of iterations\n    Iterations.append(model.best_iteration_)\n    \n    # Predictions\n    preds = np.clip(model.predict(xte), 0, 15000)\n    \n    # Print score\n    print(f\"{date_limit[:10]} :\")\n    print(f\"Train/Test : {len(xtr)}/{len(xte)} rows.\")\n    print(f\"Iterations : {Iterations[-1]}.\")\n    print(f\"Score      : {round(mean_absolute_error(yte, preds), 2)}.\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:07:24.497533Z","iopub.execute_input":"2023-11-29T07:07:24.497865Z","iopub.status.idle":"2023-11-29T07:19:42.778538Z","shell.execute_reply.started":"2023-11-29T07:07:24.497841Z","shell.execute_reply":"2023-11-29T07:19:42.776842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The number of iterations we will use for our training (trained on all data)\nprint(Iterations)\nbest_iter = int(np.median(Iterations))\nbest_iter","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:19:42.780098Z","iopub.execute_input":"2023-11-29T07:19:42.780369Z","iopub.status.idle":"2023-11-29T07:19:42.787037Z","shell.execute_reply.started":"2023-11-29T07:19:42.780347Z","shell.execute_reply":"2023-11-29T07:19:42.786182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nlgbm_params['n_estimators'] = best_iter\n\nFeature_Imp = None\n\n# Train and test\nxtr, ytr = df[feats], df['target']\nprint(f\"Train : {len(xtr)} rows.\")\n\n# Fit model\nmodel = lgb.LGBMRegressor(**lgbm_params)\nfit_params['eval_set'] = [(xtr, ytr)]\nmodel.fit(xtr, ytr, **fit_params, callbacks = [lgb.log_evaluation(50)])\n\n# Save model\njoblib.dump(model, f\"lgbm_model.pkl\")\n\n# Features Importance\nFeature_Imp = pd.DataFrame(sorted(zip(model.feature_importances_, feats)), columns=['Value','Feature'])\nFeature_Imp['Value'] = 100* (Feature_Imp['Value'] / Feature_Imp['Value'].max()) # Normalisation\nFeature_Imp = Feature_Imp.sort_values(by='Value', ascending=False).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:19:58.521378Z","iopub.execute_input":"2023-11-29T07:19:58.521905Z","iopub.status.idle":"2023-11-29T07:23:12.290395Z","shell.execute_reply.started":"2023-11-29T07:19:58.521867Z","shell.execute_reply":"2023-11-29T07:23:12.289519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features Importance\nif len(Feature_Imp) > 90 : plt.figure(figsize=(7, 15))\nelif len(Feature_Imp) > 60 : plt.figure(figsize=(7, 12))\nelif len(Feature_Imp) > 30 : plt.figure(figsize=(7, 10))\nelse :\n    plt.figure(figsize=(5, 5))\nsns.barplot(x=\"Value\", y=\"Feature\", data=Feature_Imp.head(100))\nplt.title('Features Importance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:23:52.613512Z","iopub.execute_input":"2023-11-29T07:23:52.613797Z","iopub.status.idle":"2023-11-29T07:23:53.636856Z","shell.execute_reply.started":"2023-11-29T07:23:52.613775Z","shell.execute_reply":"2023-11-29T07:23:53.635956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Most important features\nFeature_Imp.Feature.tolist()[:20]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:24:00.910727Z","iopub.execute_input":"2023-11-29T07:24:00.911089Z","iopub.status.idle":"2023-11-29T07:24:00.918018Z","shell.execute_reply.started":"2023-11-29T07:24:00.911052Z","shell.execute_reply":"2023-11-29T07:24:00.917109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Less important features\nFeature_Imp.Feature.tolist()[::-1][:10]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:24:07.755964Z","iopub.execute_input":"2023-11-29T07:24:07.756268Z","iopub.status.idle":"2023-11-29T07:24:07.761356Z","shell.execute_reply.started":"2023-11-29T07:24:07.756246Z","shell.execute_reply":"2023-11-29T07:24:07.760813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import enefit\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:24:14.611024Z","iopub.execute_input":"2023-11-29T07:24:14.611956Z","iopub.status.idle":"2023-11-29T07:24:14.642651Z","shell.execute_reply.started":"2023-11-29T07:24:14.611914Z","shell.execute_reply":"2023-11-29T07:24:14.642076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_previous_targets(test, PREVIOUS_TARGET_REVEALED) :\n    \n    for i, revealed_targets in enumerate(PREVIOUS_TARGET_REVEALED) :\n        day_shift = i + 2\n    \n        # Rename\n        revealed_targets = revealed_targets.rename(columns = {\"target\" : f\"target_revealed_{day_shift}days_ago\"})\n\n        # Add target 24h ago\n        test = pd.merge(test,\n                        revealed_targets[[\"county\", \"is_business\", \"product_type\", \"is_consumption\", f\"target_revealed_{day_shift}days_ago\", \"time\"]],\n                        how = 'left',\n                        on = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"time\"],\n                       )\n        \n    # Fill NaN with last revealed\n    for day_shift in range(2, 7) :\n        k = f\"target_revealed_{day_shift}days_ago\"\n        if k not in test :\n            test[k] = test[\"target_revealed_2days_ago\"].copy()\n        else :\n            mask = (test[k].isna())\n            test.loc[mask, k] = test.loc[mask, \"target_revealed_2days_ago\"]\n        test[k] = test[k].fillna(0.5)\n\n    # Return\n    return test\n\n#add_target_one_day_ago(test, revealed_targets)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:24:21.930706Z","iopub.execute_input":"2023-11-29T07:24:21.931474Z","iopub.status.idle":"2023-11-29T07:24:21.937476Z","shell.execute_reply.started":"2023-11-29T07:24:21.931449Z","shell.execute_reply":"2023-11-29T07:24:21.936895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reload enefit environment (only in debug mode, otherwise the submission will fail)\nif debug :\n    enefit.make_env.__called__ = False\n    type(env)._state = type(type(env)._state).__dict__['INIT']\n    iter_test = env.iter_test()\n# -------------------------------------------------------\n\n# List of target_revealed dataframes\nPREVIOUS_TARGET_REVEALED = []\n\n# Iterate and submit\nfor (test, revealed_targets, client,\n     historical_weather, forecast_weather,\n     electricity_prices, gas_prices, sample_prediction) in iter_test:\n\n    # Rename\n    client = client.rename(columns = {'date' : 'datetime'})\n    if 'datetime' not in test :\n        test   = test.rename(columns = {'prediction_datetime' : 'datetime'})\n        \n    # Boolean -> Int\n    client['is_business'] = client['is_business'].astype(int)\n    for k in ['is_business', 'is_consumption'] :\n        test[k] = test[k].astype(int)\n        revealed_targets[k] = revealed_targets[k].astype(int)\n    \n    # Date processing + features extraction\n    for date_col in ['datetime'] :\n        test = process_date(test, date_col, extract_features = True)\n        client = process_date(client, date_col)\n        historical_weather = process_date(historical_weather, date_col)\n        revealed_targets = process_date(revealed_targets, date_col)\n    for date_col in ['forecast_date', 'origin_date'] :\n        electricity_prices = process_date(electricity_prices, date_col)\n        gas_prices = process_date(gas_prices, date_col)\n    for date_col in ['forecast_datetime'] :\n        forecast_weather = process_date(forecast_weather, date_col)\n        \n    # Process weather info\n    historical_weather = process_weather_info(historical_weather)\n    forecast_weather   = process_weather_info(forecast_weather)\n    \n    # Create df\n    test = create_df(test, client, historical_weather, forecast_weather,\n                     electricity_prices, gas_prices, sample_prediction)\n    \n    # Store revealed_targets\n    PREVIOUS_TARGET_REVEALED.insert(0, revealed_targets)\n    while len(PREVIOUS_TARGET_REVEALED) > 7 :\n        PREVIOUS_TARGET_REVEALED.pop()\n    \n    # Add previous targets\n    test = add_previous_targets(test, PREVIOUS_TARGET_REVEALED)\n        \n    # In case of missing feats, we create them\n    missing_feats = [x for x in feats if x not in test]\n    if len(missing_feats) > 0 :\n        test = pd.concat([test,\n                          pd.DataFrame(0, index=np.arange(len(test)), columns=missing_feats)\n                         ],\n                         axis=1,\n                        )\n\n    # Inference\n    sample_prediction.drop(columns = ['target'], inplace=True)\n    test['target'] = np.clip(model.predict(test[feats]), 0, 15000)\n    \n    #test['target'] = 0\n    #for model in MODELS :\n    #    test['target'] += np.clip(model.predict(test[feats]), 0, 15000) / len(MODELS)\n        \n    # Add target to sample_prediction\n    sample_prediction = pd.merge(sample_prediction, test[['row_id', 'target']], on='row_id', how='left')\n    \n    # Send predictions\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:24:32.431340Z","iopub.execute_input":"2023-11-29T07:24:32.431945Z","iopub.status.idle":"2023-11-29T07:24:33.885976Z","shell.execute_reply.started":"2023-11-29T07:24:32.431897Z","shell.execute_reply":"2023-11-29T07:24:33.885158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show sample_prediction\ntry :\n    print(sample_prediction.shape)\n    display(sample_prediction.head())\nexcept :\n    print(\"Oops, something went wrong !\")","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:24:38.767038Z","iopub.execute_input":"2023-11-29T07:24:38.767359Z","iopub.status.idle":"2023-11-29T07:24:38.782141Z","shell.execute_reply.started":"2023-11-29T07:24:38.767336Z","shell.execute_reply":"2023-11-29T07:24:38.780860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}